# Unit 3: Data Science Methodology

---

**Syllabus :** *\[PEC-CSE-320G\]*                                            
**Compiled By:** *Deepak Modi (223100)*  
**Data Science Methodology:** Business Understanding, Analytic Approach, Data Requirements, Data Collection, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment, Feedback.  
---

## **Data Science Methodology**

Data Science Methodology is a **step-by-step process** used to solve real-world problems using **data**. It helps in making **data-driven decisions** by following a structured process.

### **Why is it Important?**

 âœ… Ensures a **systematic** approach to handling data.  
 âœ… Helps in making **accurate** and **reliable** predictions.  
 âœ… Increases **efficiency** in solving business problems.  
 âœ… Reduces **errors** and **bias** in decision-making.

### **Steps in Data Science Methodology**

The **Data Science Lifecycle** follows **ten major steps**:

1. Business Understanding \- Identify the problem or challenge at hand.  
2. Analytic Approach \- Determine the type of analysis required.  
3. Data Requirements \- Identify the data needed to perform the analysis.  
4. Data Collection \- Gather data from appropriate sources.  
5. Data Understanding \- Explore data for meaningful insights.  
6. Data Preparation \- Clean and preprocess data.  
7. Modeling \- Apply ML algorithms.  
8. Evaluation \- Measure model accuracy and performance.  
9. Deployment- Integrate into production.  
10. Feedback & Improvement \- Update and refine model.

### **1\. Business Understanding**

Business Understanding is the **first step in the Data Science Methodology**. It involves understanding **what problem needs to be solved**.

#### **â“ Key Questions:**

* What is the goal of the project?  
* What are we trying to solve?  
* Who are the stakeholders?  
* How will this analysis help the business?

#### **ðŸ“Œ Example:**

A telecom company wants to **predict customer churn** (i.e., customers leaving). The business goal is to **increase customer retention** by understanding the reasons behind churn.

#### **ðŸ”¹ Importance of Business Understanding**

 âœ… Ensures that the data science project aligns with **business goals**.  
 âœ… Helps in selecting **relevant data** for analysis.  
 âœ… Saves time by **focusing on the right problem**.  
 âœ… Provides **real-world value** rather than just technical insights.

#### **ðŸ”¹ Steps in Business Understanding**

##### **1ï¸âƒ£ Define the Problem**

* Clearly state the **business problem**.  
* Understand the **objectives** of the organization.  
* Identify **key stakeholders** (who will use the results).

âœ… **Example:**  
 A telecom company wants to **reduce customer churn** (customers leaving their service).

##### **2ï¸âƒ£ Identify Business Objectives**

* What is the **goal** of the analysis?  
* How will solving this problem **benefit the company**?  
* What are the **key performance indicators (KPIs)?**

âœ… **Example:**  
 The goal is to **increase customer retention** by **predicting which customers might leave** and offering them discounts or better service.

##### **3ï¸âƒ£ Understand Business Constraints**

* Are there **budget limitations**?  
* Are there **time constraints** for delivering results?  
* Are there **regulatory restrictions** on data usage?

âœ… **Example:**  
 The company has **limited customer service staff**, so they need to **target only high-risk customers** for retention offers.

##### **4ï¸âƒ£ Establish Success Criteria**

* How will we measure success?  
  What **metrics** will indicate that our model is useful?

âœ… **Example:**  
 Success is measured by:  
 âœ” **Reduced churn rate** (fewer customers leaving).  
 âœ” **Higher customer satisfaction** (positive feedback).  
 âœ” **Increased revenue** (more customers retained).

#### **ðŸ”¹ Example: Business Understanding in a Retail Store**

A **retail store** wants to **increase sales** using data science.

âœ… **Problem Statement:**  
 Sales have dropped by 15% in the last quarter.

âœ… **Business Objective:**  
 Identify **factors affecting sales** and suggest improvements.

âœ… **Constraints:**

* Data is available only for the last **12 months**.  
* Budget for new marketing campaigns is **limited**.

âœ… **Success Criteria:**

* **Increase sales by 10%** in the next 6 months.  
* **Improve customer engagement** through better promotions.

### **2\. Analytic Approach**

The **Analytic Approach** is the process of deciding **how** to solve a problem using data science techniques. It involves choosing the **type of analysis** and the **mathematical or machine learning approach** that best fits the business problem.

#### **âœ… Importance of the Analytic Approach**

 âœ” Ensures we use the **right method** to solve the problem.  
 âœ” Helps in selecting **appropriate data and tools**.  
 âœ” Determines whether we need **descriptive, predictive, or prescriptive analytics**.

#### **ðŸ”¹ Types of Analytic Approaches**

There are three main types of **analytics approaches**:

##### **1ï¸âƒ£ Descriptive Analytics â€“ â€œWhat happened?â€**

* **Summarizes past data** to understand trends.  
* Uses **charts, graphs, and reports**.  
* No predictions, only insights.

âœ… **Example:**  
 A retail store analyzes **past sales data** to see which months had the highest sales.

##### **2ï¸âƒ£ Predictive Analytics â€“ â€œWhat will happen?â€**

* Uses **historical data** to make **future predictions**.  
* Uses **machine learning models**.

âœ… **Example:**  
 A telecom company predicts **which customers are likely to churn** using past data.

##### **3ï¸âƒ£ Prescriptive Analytics â€“ â€œWhat should we do?â€**

* Suggests **best actions** to improve outcomes.  
* Uses **AI and optimization techniques**.

âœ… **Example:**  
 An **e-commerce** site recommends **discount offers** to customers based on their past purchases.

#### **ðŸ”¹ Choosing the Right Approach**

| Problem Type | Best Analytic Approach | Example |
| :---- | :---- | :---- |
| Understanding past trends | Descriptive Analytics | Sales reports of last year |
| Forecasting future events | Predictive Analytics | Predicting customer churn |
| Making data-driven decisions | Prescriptive Analytics | Suggesting best product price |

#### **ðŸ”¹ Example: Analytic Approach for Customer Churn**

ðŸ“Œ **Business Problem:** A telecom company wants to **reduce customer churn**.

ðŸ“Œ **Possible Approaches:**  
 1ï¸âƒ£ **Descriptive Analytics** â€“ Identify past churn trends. Ex: How many customers left in the last 6 months?  
 2ï¸âƒ£ **Predictive Analytics** â€“ Build a machine learning model to predict future churn. Ex: Which customers are likely to leave next month?  
 3ï¸âƒ£ **Prescriptive Analytics** â€“ Suggest loyalty offers to at-risk customers. Ex: What strategies can reduce churn?

ðŸš€ **Final Decision:** Use **Predictive Analytics** to forecast churn and then apply **Prescriptive Analytics** to take action.

### **3\. Data Requirements**

Data requirements define **what kind of data** is needed for a data science project. It ensures that the collected data is **relevant, sufficient, and of high quality** to achieve the desired outcomes.

ðŸ”¹ **Example:** If we are building a sales prediction model, we need **past sales data, customer demographics, and seasonal trends** to make accurate predictions.

#### **ðŸ”¹ Importance of Data Requirements**

âœ… Helps in collecting **only necessary data**, avoiding unnecessary storage costs.  
 âœ… Ensures the **right data types and formats** for easy analysis.  
 âœ… Helps in **better decision-making** by focusing on high-quality data.  
 âœ… Reduces **errors and biases** in machine learning models.

#### **ðŸ”¹ Key Factors in Data Requirements**

| Factor | Description | Example |
| :---- | :---- | :---- |
| **Relevance** | Data should be useful for solving the problem. | For predicting rainfall, we need weather data, not stock prices. |
| **Completeness** | Data should have no missing values. | A dataset of studentsâ€™ marks should include all subjects. |
| **Accuracy** | Data should be correct and reliable. | Customer email addresses should be valid. |
| **Consistency** | Data should follow a uniform format. | Dates should be in the same format (YYYY-MM-DD). |
| **Timeliness** | Data should be up-to-date. | Stock market predictions need recent data, not old data. |

#### **ðŸ”¹ Types of Data Required in Data Science**

1ï¸âƒ£ **Structured Data**  
 ðŸ“Œ Organized and stored in tables or databases.  
 âœ… **Example:** Excel sheets, SQL databases.  
 ðŸ”¹ **Use Case:** Sales records, customer details, financial reports.

2ï¸âƒ£ **Unstructured Data**  
 ðŸ“Œ Data that does not follow a fixed structure.  
 âœ… **Example:** Images, videos, social media posts.  
 ðŸ”¹ **Use Case:** Sentiment analysis, facial recognition, chatbots.

3ï¸âƒ£ **Quantitative Data (Numerical Data)**  
 ðŸ“Œ Data that can be measured in numbers.  
 âœ… **Example:** Age, temperature, sales figures.  
 ðŸ”¹ **Use Case:** Forecasting trends, statistical analysis.

4ï¸âƒ£ **Qualitative Data (Categorical Data)**  
 ðŸ“Œ Data that represents categories or labels.  
 âœ… **Example:** Gender (Male/Female), customer feedback (Positive/Negative).  
 ðŸ”¹ **Use Case:** Classification problems in Machine Learning.

5ï¸âƒ£ **Historical Data**  
 ðŸ“Œ Past data used for trends and pattern recognition.  
 âœ… **Example:** Last 5 years' stock market data for price prediction.

6ï¸âƒ£ **Real-time Data**  
 ðŸ“Œ Data that is continuously updated.  
 âœ… **Example:** Live traffic updates, sensor readings.

#### **ðŸ”¹ Steps to Define Data Requirements**

##### **1ï¸âƒ£ Identify the Business Problem**

ðŸ”¹ Understand the goal of the project.  
 âœ… **Example:** Predict customer churn for a telecom company.

##### **2ï¸âƒ£ Determine the Data Needed**

ðŸ”¹ Identify what data is required for the analysis.  
 âœ… **Example:** Customer usage history, complaints, and billing records.

##### **3ï¸âƒ£ Identify Data Sources**

ðŸ”¹ Decide where to collect the data from (databases, APIs, web scraping, etc.).  
 âœ… **Example:** Telecom company CRM, customer feedback surveys.

##### **4ï¸âƒ£ Define Data Format and Storage**

ðŸ”¹ Structure the data in a useful format (CSV, JSON, SQL).  
 âœ… **Example:** Store customer data in a **relational database** for easy access.

##### **5ï¸âƒ£ Ensure Data Quality and Integrity**

ðŸ”¹ Check for missing values, duplicate records, and inconsistencies.  
 âœ… **Example:** Remove incorrect customer phone numbers.

#### **ðŸ”¹ Challenges in Data Requirements**

 âš  **Incomplete Data** â€“ Missing values can lead to inaccurate predictions.  
 âš  **Data Redundancy** â€“ Duplicate records increase storage costs.  
 âš  **Privacy Concerns** â€“ Handling sensitive user data carefully (GDPR, CCPA compliance).  
 âš  **Data Compatibility Issues** â€“ Different formats (CSV vs. JSON) may require conversion.

#### **ðŸ”¹ Best Practices for Defining Data Requirements**

 âœ… **Collect only necessary data** â€“ Avoid unnecessary storage costs.  
 âœ… **Ensure high-quality data** â€“ Remove duplicates and missing values.  
 âœ… **Standardize data formats** â€“ Use consistent naming conventions.  
 âœ… **Follow legal and ethical guidelines** â€“ Protect user privacy.  
 âœ… **Regularly update data** â€“ Keep datasets fresh and relevant.

### **4\. Data Collection**

Data collection is the process of **gathering relevant data** from various sources to be used in data analysis, machine learning, or AI applications. It is a **crucial step** because the quality of a data science project depends on the **accuracy, completeness, and reliability** of the data collected.

#### **ðŸ”¹ Importance of Data Collection**

 âœ… The **accuracy** of predictions depends on high-quality data.  
 âœ… Ensures **relevant insights** by gathering the right information.  
 âœ… Helps in **detecting trends and patterns** in business.  
 âœ… Essential for **training machine learning models** effectively.

#### **ðŸ”¹ Types of Data Sources**

Data can be collected from different sources, categorized into:

| Type | Description | Examples |
| ----- | ----- | ----- |
| **Primary Data** | Data collected first-hand for a specific purpose. | Customer feedback, online surveys, IoT sensors. |
| **Secondary Data** | Pre-existing data collected by someone else. | Government reports, company databases, research papers. |
| **Structured Data** | Organized data in a fixed format (tables, databases). | Excel sheets, SQL databases, financial records. |
| **Unstructured Data** | Data without a predefined format. | Images, videos, social media posts. |
| **Real-Time Data** | Continuously generated data. | Live weather updates, stock market data, streaming logs. |

#### **ðŸ”¹ Methods of Data Collection**

### **1ï¸âƒ£ Manual Data Entry**

ðŸ“Œ Humans input data into spreadsheets, forms, or databases.  
 âœ… **Example:** Customer survey responses entered manually.

### **2ï¸âƒ£ Web Scraping**

ðŸ“Œ Extracting information from websites using scripts.  
 âœ… **Example:** Collecting product prices from e-commerce websites.  
 ðŸ”¹ **Python Example:** Scraping news headlines

**`import`** `requests`    
**`from`** `bs4 import BeautifulSoup`  

`url = "https://www.bbc.com/news"`    
`response = requests.get(url)`    
`soup = BeautifulSoup(response.text, "html.parser")`  

`headlines = soup.find_all("h3")`    
**`for`** `headline in headlines[:5]:`    
    `print(headline.text)`  

ðŸ“Œ **Use Cases:** Price monitoring, competitor analysis, stock market trends.

### **3ï¸âƒ£ API Calls**

ðŸ“Œ APIs allow us to fetch data from external services.  
 âœ… **Example:** Getting weather data from an API.  
 ðŸ”¹ **Python Example:** Fetching weather data from OpenWeather API

**`import`** `requests`  

`api_url = "http://api.openweathermap.org/data/2.5/weather?q=Delhi&appid=your_api_key"`    
`response = requests.get(api_url)`    
`data = response.json()`  

`print("Temperature:", data["main"]["temp"])`  

ðŸ“Œ **Use Cases:** Social media analytics, financial data retrieval.

### **4ï¸âƒ£ Sensor Data Collection**

ðŸ“Œ IoT devices generate real-time data from the environment.  
 âœ… **Example:** A smartwatch collecting heart rate data.  
 ðŸ”¹ **Use Cases:** Smart cities, health monitoring, industrial automation.

### **5ï¸âƒ£ Database Queries**

ðŸ“Œ Extracting data from databases using SQL queries.  
 âœ… **Example:** Getting customer details from a company database.  
 ðŸ”¹ **SQL Query Example:**

`SELECT name, email FROM customers WHERE city = 'Mumbai';`

ðŸ“Œ **Use Cases:** Business intelligence, CRM systems, transaction monitoring.

### **6ï¸âƒ£ Crowdsourcing**

 ðŸ“Œ Collecting data from a large group of people via the internet.  
 âœ… **Example:** Wikipedia edits, online surveys, citizen science projects.  
 ðŸ”¹ **Use Cases:** Image labeling for AI models, sentiment analysis datasets.

#### **ðŸ”¹ Challenges in Data Collection**

 âš  **Data Quality Issues** â€“ Incomplete, inconsistent, or incorrect data.  
 âš  **Data Privacy Concerns** â€“ Handling sensitive information responsibly.  
 âš  **Legal and Ethical Issues** â€“ Following data collection laws (GDPR, CCPA).  
 âš  **Storage and Processing Costs** â€“ Managing large volumes of data efficiently.

#### **ðŸ”¹ Best Practices for Effective Data Collection**

 âœ… **Ensure Data Accuracy** â€“ Remove duplicate or incorrect data.  
 âœ… **Use Automation** â€“ Reduce human errors with scripts and APIs.  
 âœ… **Follow Legal Regulations** â€“ Respect privacy laws and permissions.  
 âœ… **Use Secure Storage** â€“ Encrypt sensitive data to prevent leaks.  
 âœ… **Regularly Update Data** â€“ Keep the dataset fresh and relevant.

### **5\. Data Understanding**

Data Understanding is the process of **exploring and analyzing** the collected data to check its **quality, structure, and patterns** before using it for modeling.

#### **âœ… Why is Data Understanding Important?**

 âœ” Helps in identifying **missing, inconsistent, or incorrect data**.  
 âœ” Ensures that the **right data** is used for analysis.  
 âœ” Helps in **choosing the correct data preprocessing techniques**.

#### **ðŸ”¹ Steps in Data Understanding**

### **1ï¸âƒ£ Data Exploration (Basic Overview of Data)**

* Check **size and shape** of data.  
* Identify **columns (features) and rows (records)**.  
* Check **data types** (numerical, categorical, text, etc.).

âœ… **Example in Python (Using Pandas):**

**`import`** `pandas as pd`  

*`# Load dataset`*    
`df = pd.read_csv("customer_data.csv")`  

*`# Check first 5 rows`*    
`print(df.head())`  

*`# Get dataset shape (rows, columns)`*    
`print(df.shape)`  

*`# Get column names and data types`*    
`print(df.info())`  

ðŸ”¹ This helps in getting a quick overview of the dataset.

### **2ï¸âƒ£ Checking for Missing Values**

* Missing values can cause problems in analysis.  
* Identify and handle them using **imputation (filling with mean, median, mode, etc.)**.

âœ… **Example in Python:**

*`# Check for missing values`*  
`print(df.isnull().sum())`

ðŸ”¹ If missing values exist, we can **fill them** or **remove rows/columns**.

### **3ï¸âƒ£ Identifying Outliers (Extreme or Unusual Values)**

* Outliers can **distort the analysis and predictions**.  
* Use **box plots** or **statistical methods** to detect them.

âœ… **Example in Python:**

**`import`** `matplotlib.pyplot as plt`  

*`# Box plot to check for outli`*`ers`    
`df.boxplot(column=['income'])`    
`plt.show()`

ðŸ”¹ If extreme values are found, we can **remove them** or **replace them**.

### **4ï¸âƒ£ Understanding Data Distribution**

* Analyze **how data is spread** using histograms.  
* Helps in deciding **scaling or normalization techniques**.

âœ… **Example in Python:**

**`import`** `seaborn as sns`  

*`# Histogram of income column`*    
`sns.histplot(df['income'], bins=30, kde=True)`    
`plt.show()`

ðŸ”¹ Helps in understanding **skewness** and **distribution shape**.

### **5ï¸âƒ£ Finding Correlations Between Features**

* Identifies relationships between different variables.  
* Useful for **feature selection** (removing redundant features).

âœ… **Example in Python:**

*`# Correlation matrix`*    
`print(df.corr())`

*`# Heatmap visualization`*    
`sns.heatmap(df.corr(), annot=True, cmap="coolwarm")`  
`plt.show()`

ðŸ”¹ Helps in deciding which features are **important** for modeling.

#### **ðŸ”¹ Example: Data Understanding for Customer Churn Prediction**

ðŸ“Œ **Business Problem:** A telecom company wants to predict **customer churn**.

ðŸ“Œ **Steps in Data Understanding:**  
 âœ” **Check dataset size & structure** â€“ Number of customers, available features.  
 âœ” **Check missing values** â€“ Fill missing call duration values with the average.  
 âœ” **Identify outliers** â€“ Remove extreme cases of unusually high data usage.  
 âœ” **Analyze correlations** â€“ See if high call drop rates lead to churn.

#### **ðŸ”¹ Challenges in Data Understanding**

 ðŸ”´ **Large datasets** â€“ Too many records and features can be difficult to analyze.  
 ðŸ”´ **Messy data** â€“ Duplicate, inconsistent, or wrongly labeled data.  
 ðŸ”´ **Skewed distributions** â€“ Data may not be evenly distributed.  
 ðŸ”´ **Hidden patterns** â€“ Some relationships may not be obvious without visualization.

### **6\. Data Preparation**

Data Preparation is the process of **cleaning, transforming, and organizing raw data** into a format that can be used for analysis and machine learning models.

##### **âœ… Why is Data Preparation Important?**

 âœ” **Removes errors and inconsistencies** to improve accuracy.  
 âœ” **Handles missing values and outliers** to avoid bias.  
 âœ” **Transforms raw data** into a structured and meaningful format.

#### **ðŸ”¹ Steps in Data Preparation**

### **1ï¸âƒ£ Handling Missing Data**

* Missing values can **skew results** or make models inaccurate.  
* **Ways to handle missing data:**  
   âœ” **Remove missing values** (if very few).  
   âœ” **Fill (impute) missing values** with mean, median, or mode.

âœ… **Example in Python:**

**`import`** `pandas as pd`  
**`from`** `sklearn.impute import SimpleImputer`  

*`# Load dataset`*    
`df = pd.read_csv("data.csv")`  

*`# Check for missing values`*    
`print(df.isnull().sum())`  

*`# Fill missing values with mean`*    
`imputer = SimpleImputer(strategy='mean')`    
`df[['salary']] = imputer.fit_transform(df[['salary']])`  

ðŸ”¹ **Alternative:** Use median for skewed data, mode for categorical data.

### **2ï¸âƒ£ Handling Outliers**

* Outliers can **affect model performance** and **mislead analysis**.  
  **Ways to handle outliers:**  
   âœ” **Remove extreme values** if they are errors.  
   âœ” **Transform data using logarithms** to reduce impact.

âœ… **Example in Python:**

**`import`** `matplotlib.pyplot as plt`  
**`import`** `seaborn as sns`  

*`# Box plot to identify outliers`*    
`sns.boxplot(df['income'])`    
`plt.show()`  

*`# Remove outliers using IQR (Interquartile Range)`*  
`Q1 = df['income'].quantile(0.25)`  
`Q3 = df['income'].quantile(0.75)`  
`IQR = Q3 - Q1`  

`df = df[(df['income'] >= Q1 - 1.5*IQR) & (df['income'] <= Q3 + 1.5*IQR)]`

ðŸ”¹ **IQR method** keeps data within reasonable limits.

### **3ï¸âƒ£ Encoding Categorical Data**

* Machine learning models work better with **numerical data**.

* Convert **text labels** into **numbers**.

* **Methods:**  
   âœ” **Label Encoding** â€“ Assigns a number to each category.  
   âœ” **One-Hot Encoding** â€“ Creates separate columns for each category.

âœ… **Example in Python:**

**`from`** `sklearn.preprocessing import LabelEncoder, OneHotEncoder`    
*`# Label Encoding`*    
`label_encoder = LabelEncoder()`    
`df['gender'] = label_encoder.fit_transform(df['gender'])`    
*`# One-Hot Encoding`*    
`df = pd.get_dummies(df, columns=['city'], drop_first=True)`

ðŸ”¹ **Use One-Hot Encoding** for unordered categories like city names.

### **4ï¸âƒ£ Scaling and Normalization**

* Different features may have **different scales**, affecting model performance.  
* **Scaling** makes sure all features have the same importance.  
* **Methods:**  
   âœ” **Standardization** â€“ Converts data to **zero mean and unit variance**.  
   âœ” **Normalization** â€“ Converts data to a **0 to 1 range**.

âœ… **Example in Python:**

**`from`** `sklearn.preprocessing import StandardScaler, MinMaxScaler`    
*`# Standardization`*    
`scaler = StandardScaler()`    
`df[['salary', 'age']] = scaler.fit_transform(df[['salary', 'age']])`    
*`# Normalization`*    
`minmax_scaler = MinMaxScaler()`    
`df[['salary', 'age']] = minmax_scaler.fit_transform(df[['salary', 'age']])`

ðŸ”¹ **Standardization** is used for **normal distributions**, **Normalization** for skewed data.

### **5ï¸âƒ£ Splitting Data into Training and Testing Sets**

* Splitting data ensures that the model is **tested on unseen data**.  
* **Training set** â€“ Used to train the model (80%).  
* **Testing set** â€“ Used to evaluate performance (20%).

âœ… **Example in Python:**

**`from`** `sklearn.model_selection import train_test_split`  

*`# Features (X) and target variable (y)`*    
`X = df.drop(columns=['churn'])`    
`y = df['churn']`  

*`# Split data`*    
`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`  

`print("Training Data:", X_train.shape)`    
`print("Testing Data:", X_test.shape)`

ðŸ”¹ **Random State** ensures the split is reproducible.

#### **ðŸ”¹ Example: Data Preparation for House Price Prediction**

ðŸ“Œ **Business Problem:** A real estate company wants to predict **house prices**.

ðŸ“Œ **Steps in Data Preparation:**  
 âœ” **Handle Missing Values** â€“ Fill missing house sizes with the median.  
 âœ” **Remove Outliers** â€“ Remove extreme high/low prices.  
 âœ” **Encode Categorical Data** â€“ Convert house types into numbers.  
 âœ” **Scale Data** â€“ Normalize square footage and price.  
 âœ” **Split Data** â€“ 80% for training, 20% for testing.

#### **ðŸ”¹ Challenges in Data Preparation**

 ðŸ”´ **Large datasets** â€“ Cleaning millions of records is time-consuming.  
 ðŸ”´ **Messy data** â€“ Duplicates, incorrect values, and typos need correction.  
 ðŸ”´ **Imbalanced data** â€“ If one class dominates (e.g., 90% non-churn, 10% churn), predictions may be biased.

âœ… **Solution:** Use techniques like **SMOTE (Synthetic Minority Over-sampling Technique)** for balancing classes.

### **7\. Modeling**

Modeling is the process of applying **machine learning (ML) algorithms** to structured data to recognize patterns and make predictions.

##### **âœ… Why is Modeling Important?**

âœ” Helps in making **accurate predictions**.  
 âœ” Automates **decision-making**.  
 âœ” Extracts **meaningful insights** from data.

#### **ðŸ”¹ Steps in the Modeling Process**

##### **1ï¸âƒ£ Choosing the Right Model**

Different problems require different types of ML models:  
 âœ” **Regression** â†’ Predicts continuous values (e.g., house prices, stock prices).  
 âœ” **Classification** â†’ Predicts categories (e.g., spam or not spam).  
 âœ” **Clustering** â†’ Groups similar data (e.g., customer segmentation).  
 âœ” **Recommendation** â†’ Suggests products (e.g., Netflix movie recommendations).

##### **2ï¸âƒ£ Training the Model**

* **Training** involves feeding the model with **labeled data** so it can learn patterns.  
* The model **adjusts its parameters** to minimize errors.

âœ… **Example: Training a Linear Regression Model**

**`from`** `sklearn.linear_model import LinearRegression`    
**`import`** `numpy as np`  

*`# Training data (Square feet vs. Price in Lakhs)`*    
`X = np.array([[500], [1000], [1500], [2000], [2500]])`    
`y = np.array([50, 100, 150, 200, 250])`  

*`# Create and train model`*    
`model = LinearRegression()`    
`model.fit(X, y)`  

*`# Predict price for 1800 sq.ft`*    
`predicted_price = model.predict([[1800]])`    
`print(f"Predicted Price: {predicted_price[0]} Lakhs")`

ðŸ”¹ The model **learns** the relationship between square feet and price.

##### **3ï¸âƒ£ Evaluating the Model**

* Once trained, we test the modelâ€™s accuracy using **unseen test data**.  
* Common evaluation metrics:  
   âœ” **Regression** â†’ Mean Squared Error (MSE), R-squared  
   âœ” **Classification** â†’ Accuracy, Precision, Recall, F1-score

âœ… **Example: Checking Accuracy for a Classification Model**

**`from`** `sklearn.metrics import accuracy_score`    
*`# Actual vs Predicted values`*    
`y_test = [1, 0, 1, 1, 0, 1]`    
`y_pred = [1, 0, 1, 0, 0, 1]`  

*`# Calculate accuracy`*    
`accuracy = accuracy_score(y_test, y_pred)`    
`print(f"Model Accuracy: {accuracy * 100:.2f}%")`

ðŸ”¹ Higher accuracy means a **better model**.

##### **4ï¸âƒ£ Hyperparameter Tuning**

* Hyperparameters control how the model learns (e.g., learning rate, number of trees).  
* Adjusting them **improves model performance**.

âœ… **Example: Using GridSearchCV for Tuning**

**`from`** `sklearn.model_selection import GridSearchCV`    
**`from`** `sklearn.ensemble import RandomForestClassifier`  

*`# Define model and parameters`*    
`model = RandomForestClassifier()`    
`params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}`  

*`# Perform Grid Search`*    
`grid_search = GridSearchCV(model, params, cv=5)`    
`grid_search.fit(X_train, y_train)`  

`print("Best Parameters:", grid_search.best_params_)`

ðŸ”¹ Finds the **best combination of hyperparameters** for better performance.

##### **5ï¸âƒ£ Deploying the Model**

* Once trained and optimized, the model is deployed in **real-world applications**.  
* Example: Predicting **loan approvals** in banking, **disease diagnosis** in healthcare.

âœ… **Example: Saving and Loading a Trained Model**

**`import`** `joblib`  

*`# Save trained model`*    
`joblib.dump(model, "final_model.pkl")`  

*`# Load model later`*    
`loaded_model = joblib.load("final_model.pkl")`  

ðŸ”¹ Allows the model to be **used anytime without retraining**.

#### **ðŸ”¹ Example: Modeling for Customer Churn Prediction**

ðŸ“Œ **Business Problem:** A telecom company wants to predict **which customers will leave**.

ðŸ“Œ **Steps in Modeling:**  
 âœ” **Choose Model** â€“ Logistic Regression (since churn is Yes/No).  
 âœ” **Train Model** â€“ Feed past customer data into the model.  
 âœ” **Evaluate** â€“ Check accuracy using test data.  
 âœ” **Fine-Tune** â€“ Adjust parameters for better performance.  
 âœ” **Deploy** â€“ Use the model to **predict future churn cases**.

#### **ðŸ”¹ Challenges in Modeling**

ðŸ”´ **Overfitting** â€“ Model learns **too much** from training data and performs poorly on new data.  
 ðŸ”´ **Underfitting** â€“ Model is **too simple** and misses important patterns.  
 ðŸ”´ **Imbalanced Data** â€“ One class (e.g., fraud cases) dominates the dataset, making predictions biased.

âœ… **Solutions:**  
 âœ” Use **cross-validation** to avoid overfitting.  
 âœ” Try **different algorithms** to improve accuracy.  
 âœ” **Balance data** using oversampling (SMOTE).

### **8\. Evaluation in Data Science Methodology**

#### **ðŸ”¹ What is Model Evaluation?**

Model evaluation is the process of **measuring the performance** of a trained machine learning model. It helps us understand:  
 âœ” **How well the model is performing** on unseen data.  
 âœ” **Whether it is overfitting or underfitting.**  
 âœ” **If improvements are needed** before deployment.

#### **ðŸ”¹ Why is Evaluation Important?**

âœ… Ensures the model makes **accurate predictions**.  
 âœ… Helps in **choosing the best model**.  
 âœ… Prevents **errors in real-world applications**.

#### **ðŸ”¹ Evaluation Process**

Before evaluation, we divide the dataset into:  
 âœ” **Training Set** â†’ Used to train the model.  
 âœ” **Testing Set** â†’ Used to evaluate the modelâ€™s performance.

âœ… **Example: Splitting Data (80% Training, 20% Testing)**

**`from`** `sklearn.model_selection import train_test_split`    
**`from`** `sklearn.datasets import load_iris`  

*`# Load dataset`*    
`iris = load_iris()`    
`X, y = iris.data, iris.target`  

*`# Split dataset`*    
`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`  

`print("Training Data Shape:", X_train.shape)`    
`print("Testing Data Shape:", X_test.shape)`

ðŸ”¹ This prevents the model from seeing the test data **during training**, ensuring fair evaluation.

#### **ðŸ”¹ Metrics for Model Evaluation**

Different types of models require different evaluation metrics.

##### **ðŸ“Œ A. Regression Models (Predicting Continuous Values)**

Regression models predict numerical values (e.g., **house prices, stock prices**).

#### **âœ… Common Evaluation Metrics:**

âœ” **Mean Squared Error (MSE)** â€“ Measures average squared difference between actual and predicted values.  
 âœ” **Root Mean Squared Error (RMSE)** â€“ Square root of MSE for easier interpretation.  
 âœ” **Mean Absolute Error (MAE)** â€“ Average absolute difference between actual and predicted values.  
 âœ” **RÂ² Score (Coefficient of Determination)** â€“ Measures how well the model explains variation in data.

âœ… **Example: Evaluating a Regression Model**

**`from`** `sklearn.metrics import mean_squared_error, r2_score`    
**`from`** `sklearn.linear_model import LinearRegression`    
**`import`** `numpy as np`  

*`# Sample data`*    
`X = np.array([[1], [2], [3], [4], [5]])`    
`y = np.array([2, 4, 6, 8, 10])`  

*`# Train model`*    
`model = LinearRegression()`    
`model.fit(X, y)`  

*`# Predictions`*    
`y_pred = model.predict(X)`  

*`# Evaluation`*    
`mse = mean_squared_error(y, y_pred)`    
`r2 = r2_score(y, y_pred)`  

`print(f"MSE: {mse}")`    
`print(f"RÂ² Score: {r2}")`

ðŸ”¹ **Lower MSE \= Better Model**, **Higher RÂ² \= Better Fit**.

##### **ðŸ“Œ B. Classification Models (Predicting Categories \- Yes/No, Spam/Not Spam)**

Classification models predict categories (e.g., **pass/fail, spam/not spam**).

#### **âœ… Common Evaluation Metrics:**

âœ” **Accuracy** â€“ Measures overall correctness of predictions.  
 âœ” **Precision** â€“ Measures how many positive predictions were correct.  
 âœ” **Recall (Sensitivity)** â€“ Measures how many actual positives were predicted correctly.  
 âœ” **F1-Score** â€“ Balances precision and recall (best when classes are imbalanced).  
 âœ” **Confusion Matrix** â€“ Shows TP, FP, TN, FN.

âœ… **Example: Evaluating a Classification Model**

from sklearn.metrics import accuracy\_score, classification\_report, confusion\_matrix    
from sklearn.linear\_model import LogisticRegression  

*`# Sample data`*    
`X_train = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]`    
`y_train = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`  

*`# Train model`*    
`model = LogisticRegression()`    
`model.fit(X_train, y_train)`  

*`# Test data`*    
`X_test = [[4.5], [7.5]]`    
`y_test = [1, 1]`  

*`# Predictions`*    
`y_pred = model.predict(X_test)`  

*`# Evaluation`*    
`print("Accuracy:", accuracy_score(y_test, y_pred))`    
`print("Classification Report:\n", classification_report(y_test, y_pred))`    
`print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))`

ðŸ”¹ **Higher Accuracy, Precision, Recall, and F1-Score \= Better Model**.

##### **ðŸ”¹ Overfitting vs Underfitting**

| Issue | What Happens? | Solution |
| ----- | ----- | ----- |
| **Overfitting** | Model learns training data too well but fails on new data | Use **cross-validation**, **regularization**, **more data** |
| **Underfitting** | Model is too simple and doesnâ€™t capture patterns | Use a **complex model**, add **more features**, **train longer** |

âœ… **Solution: Use Cross-Validation**

**`from`** `sklearn.model_selection import cross_val_score`    
**`from`** `sklearn.tree import DecisionTreeClassifier`  

*`# Train model using Cross-Validation`*    
`model = DecisionTreeClassifier()`    
`scores = cross_val_score(model, X_train, y_train, cv=5)`  

`print("Cross-Validation Scores:", scores)`    
`print("Average Score:", scores.mean())`

ðŸ”¹ Cross-validation **prevents overfitting** by testing on different subsets.

### **9\. Deployment in Data Science Methodology**

#### **ðŸ”¹ What is Deployment?**

Deployment is the process of **integrating a trained machine learning model** into a real-world system so that users can interact with it and make predictions on new data.

âœ… **Example Scenarios:**  
 âœ” A **recommendation system** in an e-commerce website suggesting products.  
 âœ” A **fraud detection model** running in a banking system.  
 âœ” A **spam filter** used in email services.

##### **ðŸ”¹ Why is Deployment Important?**

âœ” **Brings the model into real-world applications**  
 âœ” **Allows users to make predictions on live data**  
 âœ” **Helps businesses automate tasks and improve decision-making**

#### **ðŸ”¹ Deployment Process**

##### **1ï¸âƒ£ Choose the Right Deployment Strategy**

There are different ways to deploy a machine learning model:

| Deployment Type | Description | Example |
| ----- | ----- | ----- |
| **Batch Processing** | Model runs on a schedule to process data in bulk | Fraud detection on bank transactions every night |
| **Real-Time API** | Model provides instant predictions via an API | Chatbot responding to customer queries |
| **Edge Deployment** | Model runs on local devices (mobile, IoT) | AI in smart cameras or fitness trackers |
| **Embedded in Applications** | Model is integrated directly into software | Spam filter inside an email service |

âœ… **Example: Deploying a Spam Filter as a Web API**  
 ðŸ“Œ The model will predict if an email is spam or not in real-time.

##### **2ï¸âƒ£ Save the Trained Model**

Before deployment, the trained model needs to be saved so it can be reused.  
 âœ… **Saving Model Using Joblib**

**`import`** `joblib`  

*`# Save model`*    
`joblib.dump(model, "spam_classifier.pkl")`  

`âœ… Load the Model When Needed`  
*`# Load saved model`*    
`loaded_model = joblib.load("spam_classifier.pkl")`  

##### **3ï¸âƒ£ Deploy as a REST API Using Flask**

A **REST API** allows applications to send data to the model and receive predictions.

âœ… **Install Flask:**

`pip install flask`

âœ… **Create an API (app.py):**

**`from`** `flask import Flask, request, jsonify`    
**`import`** `joblib`  

*`# Load trained model`*    
`model = joblib.load("spam_classifier.pkl")`  

`app = Flask(__name__)`  

`@app.route('/predict', methods=['POST'])`    
**`def`** `predict():`    
    `data = request.get_json()  # Get input data`    
    `prediction = model.predict([data['email_text']])  # Predict`    
    `return jsonify({"Spam Prediction": "Spam" if prediction[0] == 1 else "Not Spam"})`  

**`if`** `__name__ == '__main__':`    
    `app.run(debug=True)`  

âœ… **Run the API:**

`python app.py`

ðŸ“Œ Now, the model can receive email text and classify it as spam or not spam.

##### **4ï¸âƒ£ Deploy the API on Cloud (AWS, Heroku, or Vercel)**

After creating the API, deploy it on cloud platforms like:  
 âœ” **AWS Lambda** â€“ Serverless deployment for real-time predictions.  
 âœ” **Google Cloud AI Platform** â€“ Scalable AI model hosting.  
 âœ” **Heroku / Vercel** â€“ Quick API deployment.

âœ… Deploy on Heroku (Example)  
 1ï¸âƒ£ Install Heroku CLI  
`pip install gunicorn`

2ï¸âƒ£ Create a Procfile:  
`web: gunicorn app:app`

3ï¸âƒ£ Deploy to Heroku

`git init`  
`git add .`  
`git commit -m "Deploy ML API"`  
`heroku create my-ml-api`  
`git push heroku master`

Now, the API is **live and accessible**.

##### **5ï¸âƒ£ Monitor Model Performance**

After deployment, monitor the model to ensure accuracy remains high.  
 âœ… **Key Monitoring Metrics:**  
 âœ” **Response Time** â€“ How fast predictions are made.  
 âœ” **Accuracy Drift** â€“ If predictions become less accurate over time.  
 âœ” **User Feedback** â€“ If users are satisfied with model predictions.

ðŸ“Œ Example: Store user feedback to improve future models.

##### **6ï¸âƒ£ Update and Retrain the Model**

As new data comes in, retrain the model periodically.  
 âœ… **Automate Retraining with New Data:**

*`# Load new data`*  
`new_data = load_new_data()`

*`# Retrain model`*  
`model.fit(new_data['features'], new_data['labels'])`

*`# Save updated model`*  
`joblib.dump(model, "updated_model.pkl")`

ðŸ”¹ **Schedule retraining every few weeks** to improve predictions.

### **10\. Feedback & Improvement**

#### **ðŸ”¹ What is Feedback in Data Science?**

Feedback is the process of **monitoring the performance of a deployed machine learning model and improving it** based on new data, user feedback, and real-world performance.

âœ… **Why is Feedback Important?**  
 âœ” Ensures the model remains **accurate and relevant** over time.  
 âœ” Helps in **detecting biases, errors, and performance drops**.  
 âœ” Enables **continuous learning and model updates**.

#### **ðŸ”¹ Types of Feedback in Data Science**

| Type | Description | Example |
| ----- | ----- | ----- |
| **User Feedback** | Users provide ratings, corrections, or suggestions. | A spam filter incorrectly marks an important email as spam, and the user marks it as "Not Spam". |
| **Model Performance Metrics** | Tracking accuracy, precision, recall, and errors over time. | An e-commerce recommendation system's accuracy drops from 85% to 70%. |
| **Real-Time Monitoring** | Checking response times, system errors, and load handling. | A chatbot model starts responding slowly during high traffic hours. |
| **Data Drift Detection** | Identifying changes in incoming data patterns. | A fraud detection model trained on 2023 transaction data performs poorly in 2025 due to changes in fraud tactics. |

#### **ðŸ”¹ Feedback Collection Methods**

##### **1ï¸âƒ£ User Feedback Mechanisms**

ðŸ”¹ Allow users to give direct input on predictions.  
 âœ… **Example: Getting User Feedback on a Spam Filter**

`feedback = input("Was this email correctly classified? (yes/no): ")`    
**`if`** `feedback == "no":`    
    `print("Thank you! We will improve the model.")`  

ðŸ“Œ This helps collect **real-world corrections** to improve future predictions.

##### **2ï¸âƒ£ Monitoring Model Performance**

âœ… **Track key metrics regularly:**  
 âœ” **Accuracy** â€“ Is the model still predicting correctly?  
 âœ” **Response Time** â€“ Is the system getting slower?  
 âœ” **Error Rate** â€“ Are incorrect predictions increasing?

âœ… **Example: Monitoring Accuracy Drop in a Classification Model**

**`from`** `sklearn.metrics import accuracy_score`  

*`# Actual labels vs. Predicted labels`*    
`y_actual = [1, 0, 1, 1, 0, 0, 1]`    
`y_predicted = [1, 0, 1, 0, 0, 1, 1]`  

*`# Calculate accuracy`*    
`accuracy = accuracy_score(y_actual, y_predicted)`    
`print(f"Current Model Accuracy: {accuracy * 100:.2f}%")`  

ðŸ“Œ If accuracy **drops significantly**, itâ€™s time to **retrain the model**.

##### **3ï¸âƒ£ Detecting Data Drift**

ðŸ”¹ **Data drift** occurs when the statistical properties of incoming data **change over time**, making the model less effective.  
 âœ… **Example: Checking If New Data Has Different Distribution**

**`import`** `numpy as np`    
**`from`** `scipy.stats import ks_2samp`  

*`# Old data distribution`*    
`old_data = np.random.normal(50, 10, 1000)`  

*`# New incoming data`*    
`new_data = np.random.normal(60, 10, 1000)`  

*`# Perform Kolmogorov-Smirnov test`*    
`stat, p_value = ks_2samp(old_data, new_data)`    
**`if`** `p_value < 0.05:`    
    `print("Data drift detected! Need model retraining.")`    
**`else`**`:`    
    `print("No significant data drift detected.")`  

ðŸ“Œ If data drift is detected, **collect new data** and **retrain the model**.

#### **ðŸ”¹ Improving the Model Based on Feedback**

##### **1ï¸âƒ£ Retraining the Model with New Data**

âœ… **Steps to Improve a Model:**  
 1ï¸âƒ£ Collect feedback and new data.  
 2ï¸âƒ£ **Clean & preprocess** new data.  
 3ï¸âƒ£ Retrain the model.  
 4ï¸âƒ£ Evaluate and compare with the old model.  
 5ï¸âƒ£ Deploy the improved model.

`âœ… Example: Retraining a Model on New Data`  
*`# Load new dataset`*  
`new_data = load_new_data()`

*`# Train model again`*  
`model.fit(new_data['features'], new_data['labels'])`

*`# Save the updated model`*  
`joblib.dump(model, "updated_model.pkl")`

ðŸ“Œ This ensures the model **adapts to new trends and patterns**.

##### **2ï¸âƒ£ Hyperparameter Tuning for Better Performance**

Instead of training the same model, adjust **hyperparameters** to find the best-performing version.  
 âœ… **Example: Using Grid Search for Hyperparameter Tuning**

**`from`** `sklearn.model_selection import GridSearchCV`    
**`from`** `sklearn.ensemble import RandomForestClassifier`  

*`# Define hyperparameters`*    
`param_grid = {`    
    `'n_estimators': [10, 50, 100],`    
    `'max_depth': [None, 10, 20],`    
`}`  

*`# Perform Grid Search`*    
`grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)`    
`grid_search.fit(X_train, y_train)`  

`print("Best Parameters:", grid_search.best_params_)`  

ðŸ“Œ This helps **optimize model performance** without collecting new data.

##### **3ï¸âƒ£ A/B Testing for Model Comparison**

Instead of replacing a model immediately, run **two versions** side by side.  
 âœ… **Example: Testing a New Model Against the Old One**

**`from`** `sklearn.metrics import f1_score`  

*`# Old and New Model Predictions`*    
`y_old_pred = old_model.predict(X_test)`    
`y_new_pred = new_model.predict(X_test)`  

*`# Compare F1-Scores`*    
`old_f1 = f1_score(y_test, y_old_pred, average='weighted')`    
`new_f1 = f1_score(y_test, y_new_pred, average='weighted')`  

**`if`** `new_f1 > old_f1:`    
    `print("New model performs better! Deploying it now.")`    
**`else`**`:`    
    `print("Old model is still better. No change needed.")`  

ðŸ“Œ **Choose the best model** based on performance.

#### **ðŸ”¹ Automating Feedback & Model Improvement**

Instead of manually monitoring performance, automate the process.  
 âœ… **Example: Using a Scheduled Job for Model Retraining**

**`import`** `schedule`    
**`import`** `time`  

**`def`** `retrain_model():`    
    `print("Retraining model...")`    
    `new_data = load_new_data()`    
    `model.fit(new_data['features'], new_data['labels'])`    
    `joblib.dump(model, "updated_model.pkl")`    
    `print("Model retrained and saved!")`  

*`# Schedule retraining every week`*    
`schedule.every().week.do(retrain_model)`  

**`while`** `True:`    
    `schedule.run_pending()`    
    `time.sleep(3600)  # Check every hour`  

ðŸ“Œ This ensures the **model stays updated** without manual intervention.