# Unit 1: Compilers and Lexical Analysis

**Syllabus :** *\[PCC-CSE-302G\]*                                           
**Compiled By:** *Deepak Modi (223100)*  

**Introduction to Compilers:** Language Processors, The Structure of compiler: its different phases, Compiler Construction Tools, Applications of Compiler Technology.   

**Lexical Analysis:** Role of lexical analyzer, Input Buffering, Specification and recognition of tokens, design of lexical analyzer, regular expressions, A language specifying lexical analyzer, Finite automata, conversion from regular expression to finite automata, and vice versa, minimizing number of states of DFA, Implementation of lexical analyzer.   

---

## **Section 1 : Introduction to Compilers**

### **1.1 Language Processors**

A **Language Processor** is a system software that helps in converting one form of language (usually high-level programming language) into another (like machine code) These are essential for developing and running programs, as computers can only understand machine-level instructions (binary code).

Diagrammatically, a language processor can be represented as:
```
+---------------------+
| High-Level Language | (Source Code)
+---------------------+
          |
          |  (Language Processor)
          v
+---------------------+
|   Machine Code      | (Executable Code)
+---------------------+
            |
            |  (Execution)
            v   
+---------------------+
|  Computer Hardware  |
+---------------------+
```

#### **Types of Language Processors:**

1. **Compiler**  
   * Translates the **entire source code** (high-level program) into machine code **before execution**.  
   * Faster execution compared to an interpreter.  
   * Example: C, C++, and Java compilers.  
2. **Interpreter**  
   * Translates and executes the program code **line by line**.  
   * Slower than compilers but useful for debugging.  
   * Example: Python, JavaScript use interpreters.  
3. **Assembler**  
   * Converts **assembly language** (low-level human-readable code) into **machine code** (binary format).  
   * Example: MASM (Microsoft Macro Assembler) for x86 assembly.

#### **Compiler:**

A **compiler** is a program that converts source code written in a high-level language (C, Java, etc.) into machine code (binary format) that a computer can understand.

##### **Features of a Compiler:**

 * Translates the entire code at once.  
 * Provides a list of errors after compilation.  
 * Generates optimized executable machine code.  
 * Improves execution speed compared to interpreters.

### **1.2 Language Processing System**

A **Language Processing System** is a collection of tools that convert a **high-level programming language** into **executable machine code.** It ensures that programs are **translated correctly, optimized for performance, and ready for execution**.

* It consists of various components such as **Preprocessors**, **Compilers, Interpreters, Assemblers, Linkers, and Loaders**.  
* The entire process involves **multiple stages**, each performing a specific task to generate the final executable file.

#### **Components of the Language Processing System**

```
High-Level Source Code (Input)
          |
+---------‚ñº-----------+
|   Preprocessor      | (Handles macros, file inclusion)
+---------------------+
          |  Preprocessed Source code
+---------‚ñº-----------+
|     Compiler        | (Translates high-level code to assembly)
+---------------------+
          |  Assembly Code
+---------‚ñº-----------+
|     Assembler       | (Converts assembly code to machine code)
+---------------------+
          |  Machine Code (Object File)
+---------‚ñº-----------+
|     Linker          | (Links object files and libraries)
+---------------------+
          |  Binary Code (Executable File)
+---------‚ñº-----------+
|     Loader          | (Loads executable into memory)
+---------------------+
          |
          ‚ñº
Execution of Program (Output)
```



#### **Working of the Language Processing System**

The **step-by-step process** of converting source code into an executable file:

`High-Level Code ‚Üí Preprocessing ‚Üí Compilation ‚Üí Assembly ‚Üí Linking ‚Üí Loading ‚Üí Execution`

##### **Step 1: High-Level Code (Source Code)**

* The programmer writes code in a **high-level language** such as **C, Java, or Python**.  
 * **Example:**  
`int sum = a + b;`

##### **Step 2: Preprocessing**

* The **preprocessor** processes the source code before compilation.  
 * Handles **macro expansion** and **file inclusion** (`#include` in C).  
 * Removes **comments** and expands **preprocessor directives**.  
 * **Example:**  
`#include <stdio.h>`  
`#define PI 3.14`

* The preprocessor **replaces** `#define PI 3.14` wherever `PI` is used.

##### **Step 3: Compilation**

* The **compiler** translates the preprocessed source code into **assembly language**.  
 * **Checks for syntax errors**.  
 * Generates the assembly code.

* **Compiler vs. Interpreter**

* The Compiler converts the high-level code into assembly language or machine code.   
* The Interpreter executes the code line by line without converting it to machine code first.

| Feature | Compiler | Interpreter |
| ----- | ----- | ----- |
| **Execution** | Translates **entire program** before running. | Translates & executes **line by line**. |
| **Speed** | Faster execution. | Slower execution. |
| **Example Languages** | C, C++ | Python, JavaScript |

##### **Step 4: Assembly**

üìå The **assembler** converts assembly code into **machine code (binary format)**.  
 ‚úî The output is an **object file** (`.o` or `.obj`).  
 ‚úî This file contains **machine or low-level instructions** understandable by the processor.

üîπ **Example of Assembly Code ‚Üí Machine Code**  
`MOV R1, a`    
`ADD R1, b`    
`MOV sum, R1`  
‚úî This is **translated into binary (machine code)**:  
`01010110 11001001 10000010`

##### **Step 5: Linking**

üìå The **linker** combines multiple object files and **resolves function calls**.  
 ‚úî Links **library functions** (e.g., `printf()` from `stdio.h`).  
 ‚úî Produces a **final executable file** (`.exe`, `.out`).

üîπ **Example:**  
 If a program calls `printf()`, the linker **fetches its definition** from the C standard library (`libc`).

##### **Step 6: Loading**

üìå The **loader** loads the **executable file** into **RAM** and hands over control to the **CPU**.  
 ‚úî Assigns memory space for **variables and functions**.  
 ‚úî Starts **program execution**.

üîπ **Example Execution:**

`User clicks "Run" ‚Üí Loader loads the program into RAM ‚Üí CPU executes instructions.`

### **1.3 Structure of a Compiler**

A compiler works in multiple phases, which are broadly divided into **two main parts**:

#### **I. Analysis Phase (Front-End)**

The front-end processes the source code and checks for correctness.

1. **Lexical Analysis**: Converts source code into tokens (keywords, identifiers, literals).  
2. **Syntax Analysis (Parsing)**: Checks whether the token sequence follows grammar rules.  
3. **Semantic Analysis**: Checks for logical errors (e.g., type mismatches).

#### **II. Synthesis Phase (Back-End)**

The back-end generates optimized machine code.

4. **Intermediate Code Generation**: Converts code into an intermediate form (Three-Address Code).  
5. **Code Optimization**: Improves performance by reducing execution time and memory usage.  
6. **Code Generation**: Translates optimized intermediate code and produces final machine code (binary).

#### **Compiler Structure Diagram**

```plaintext
Input Source Code (High Level)
          | Characters
+---------‚ñº-----------+
|   Lexical Analysis  |
|   (Tokenization)    |
+---------------------+
          | Tokens
+---------‚ñº-----------+
|   Syntax Analysis   |
|     (Parsing)       |
+---------------------+
          | Parse / Syntax Tree
+---------‚ñº-----------+
|   Semantic Analysis |
|   (Type Checking)   |
+---------------------+
          | Abstract Syntax / Semantic Tree
+---------‚ñº-----------+
| Intermediate Code   |
|    Generation       |
+---------------------+
          | Intermediate Code (TAC)
+---------‚ñº-----------+
|   Code Optimization |
+---------------------+
          | Optimized Intermediate Code
+---------‚ñº-----------+
|   Code Generation   |
+---------------------+
          | Assembly Code
          ‚ñº
Executable Machine Code (Low-Level)
```

#### **Phases of a Compiler**

A **compiler** is a program that translates source code written in a high-level language into assembly code or machine code. The compilation process is divided into **six main phases**, supported by components like **symbol table** and **error handling**.

| Phase | Input | Process | Output |
| :---: | :---: | :---: | :---: |
| **Lexical Analysis** | Source Code | Tokenization, comment removal | Tokens |
| **Syntax Analysis** | Tokens | Grammar checking, Parse Tree creation | Parse Tree |
| **Semantic Analysis** | Parse Tree | Type and scope checking | Validated AST |
| **Intermediate Code Gen.** | Valid AST | Convert to IR | Intermediate Code |
| **Code Optimization** | Intermediate Code | Remove redundancies, speed up code | Optimized Code |
| **Code Generation** | Optimized Code | Generate machine-level code | Machine Code |
| **Error Handling** | All phases | Detect & report errors | Error messages |
| **Symbol Table** | Source Identifiers | Store and manage symbol info | Symbol metadata (name, type) |

##### **1\. Lexical Analysis (Tokenization)**

* **Input:** Raw source code || **Output:** Sequence of **tokens**  
* Reads **characters** from source code.  
* Groups them into **tokens** (keywords, identifiers, literals, operators).  
* Removes **whitespace and comments**.

Example:   
**For input:** `int sum = a + 1;`  
**Output Tokens:** `int`, `sum`, `=`, `a`, `+`, `1`, `;`

##### **2\. Syntax Analysis (Parsing)**

* **Input:** Token stream || **Output:** Parse Tree / Syntax Tree  
* Checks if the sequence of tokens follows **grammatical rules** of the language.  
* Creates a **Parse Tree** or **Abstract Syntax Tree (AST)**.  
* Detects **syntax errors** (e.g., missing semicolon).

Example (Valid & Invalid Syntax in C):  
`int x = 10 + ;  // ‚ùå Error (invalid syntax)`  
`int x = 10 + 5; // ‚úÖ Correct syntax`

üìå **Example:** Syntax tree for `int sum = a + b;`

      `=`  
     `/ \`  
  `sum   +`  
       `/ \`  
      `a   b`

üöÄ **If syntax is incorrect**, it reports **syntax errors** (e.g., missing semicolon).

##### **3\. Semantic Analysis**

* **Input:** Syntax Tree || **Output:** Annotated syntax tree (with semantic info)  
* Checks **meaning of statements** (logical correctness).  
* Checks **logical errors** (e.g., type mismatches, scope resolution, undefined variables).  
* Uses **Symbol Table** to verify variable declarations and types.

Example (Valid & Invalid Semantics in C):  
`int sum = "ten";  // ‚ùå Type mismatch (String assigned to int)`  
`int sum = 10;      // ‚úÖ Correct`

üöÄ **If semantic errors exist, compilation stops here.**

##### **4\. Intermediate Code Generation**

* **Input:** Validated syntax/semantic tree || **Output:** Intermediate Code (e.g., TAC or AST)  
* Converts code into an **intermediate representation** (IR) that is **independent of machine architecture**. Easier to optimize and port between systems.  
* Common forms:  
    * **Three-Address Code (TAC)**  
    * **Quadruples**
    * **Triples**
* Example:  
  `int sum = a + b;`

‚û°Ô∏è Intermediate Code:  
`T1 = a + b`  
`sum = T1`

##### **5\. Code Optimization**

* **Input:** Intermediate Code || **Output:** Optimized Intermediate Code  
* Improves efficiency by reducing **execution time** or **memory usage**.  
* Removes **redundant computations** and **dead code**.  
* Performs optimizations like **constant folding**, **dead code elimination**, **loop unrolling**, etc.

üìå **Example:**

Before optimization: int a = 10 * 2;  // Computed at runtime  
After optimization: int a = 20;  // Computed at compile-time (constant folding)

üöÄ **This step is crucial for performance in large programs.**

##### **6\. Code Generation**

* **Input:** Optimized Intermediate Code || **Output:** Executable machine code  
* Converts optimized code into **machine code** (binary).  
* Converts **intermediate code ‚Üí assembly code ‚Üí machine code**.  
* Allocates **registers** and **memory locations** for variables.

**Example:** `sum = a + b;`  
The generated assembly code might be:  
`MOV R1, a`  
`ADD R1, b`  
`MOV sum, R1`

üöÄ This final step produces an executable file.

##### **7\. Error Handling**

* Detects and manages errors in all phases:  
  * **Lexical Errors:** Invalid characters  
  * **Syntax Errors:** Grammar violations  
  * **Semantic Errors:** Type mismatches, undeclared variables  
  * **Runtime Errors:** Division by zero  
* **Input:** Output of each phase  
* **Output:** Error messages and recovery actions

üìå **Examples:**

`int @var = 5;       // ‚ùå Lexical Error ‚Äì invalid character`    
`printf("Hello);     // ‚ùå Syntax Error ‚Äì missing quote`  
`if (x > 10) { ... } // ‚ùå Semantic Error ‚Äì undeclared variable`    
`int a = 5 / 0;      // ‚ùå Runtime Error ‚Äì division by zero`

üöÄ  Compilation stops or error is reported for correction.

##### **8\. Symbol Table**

* Data structure used throughout compilation to store metadata of identifiers like Variable/function names, Data types, Scope and memory locations.  
* **Used by:** Semantic Analyzer, Intermediate Code Generator, Optimizer, Code Generator.

üìå **Example:**

`int x = 5;`

‚û°Ô∏è Symbol Table:

| Name | Type | Scope | Memory Location |
| :---: | :---: | :---: | :---: |
| x | int | main | 0x1000 |

### **1.4 Compiler Construction Tools:**

The compilation process is divided into **6 main phases** (along with supporting components like error handling and symbol table). Each phase can use specific tools to ease the compiler development process.

These Compiler construction tools help automate different phases of compiler development.

| Phase | Purpose | Tool / Technique Used | Full Form |
| ----- | ----- | ----- | ----- |
| **1\. Lexical Analysis** | Convert source code into tokens | **Lex / Flex** | Lex \= *Lexical Analyzer Generator* Flex \= *Fast Lexical Analyzer Generator* |
| **2\. Syntax Analysis** | Check grammar structure and generate parse tree | **Yacc / Bison**, **Parser Generators (ANTLR, JavaCC, CUP)** | Yacc \= *Yet Another Compiler Compiler* Bison \= *GNU Parser Generator* ANTLR \= *Another Tool for Language Recognition* JavaCC \= *Java Compiler Compiler* CUP \= *Constructor of Useful Parsers* |
| **3\. Semantic Analysis** | Ensure logical correctness | **Syntax-Directed Translation (SDT)**, **Symbol Table Manager** | SDT \= *Syntax-Directed Translation* |
| **4\. Intermediate Code Generation** | Convert syntax tree into IR | **Three-Address Code (TAC)** Generator, SDT Engines | TAC \= *Three-Address Code* IR \= *Intermediate Representation* |
| **5\. Code Optimization** | Optimize code for performance | **LLVM**, Custom Optimization Algorithms (e.g., CFG, DFA) | LLVM \= *Low Level Virtual Machine* |
| **6\. Code Generation** | Generate machine/assembly code | **LLVM**, **GCC Backend**, Code Generator Modules | GCC \= *GNU Compiler Collection* |
|  **Supporting Tools** |  |  |  |
| **Symbol Table Manager** | Store and manage identifiers | Hash Tables, Tries, Trees | ‚Äî |
| **Error Handling Tools** | Catch and recover from errors in all phases | Embedded in Lex/Yacc/Parsers | ‚Äî |

#### **Common Compiler Construction Tools**

##### **1\. Lexical Analyzer Generators**

**Tools:** `Lex`, `Flex`

* These tools help generate a **Lexical Analyzer** (also called a scanner or tokenizer).  
* They read **regular expressions** and generate C code to match tokens.  
* **Input:** Token patterns (like identifiers, keywords, numbers).  
* **Output:** C code that scans and produces tokens from source code.

**Use in Compiler:** Converts source code into tokens for the parser.

##### **2\. Parser Generators**

**Tools:** `Yacc`, `Bison`, `ANTLR`, `JavaCC`

* These tools generate **Syntax Analyzers** (parsers) from **Context-Free Grammars (CFGs)**.  
* **Yacc/Bison:** For C/C++ code, supports LALR parsing.  
* **ANTLR:** Advanced parser supporting LL(\*) grammars, widely used in Java.  
* **JavaCC:** Parser generator for Java applications.

**Use in Compiler:** Builds parse tree or abstract syntax tree (AST) by analyzing token structure.

##### **3\. Syntax-Directed Translation Engines**

**Used in:** Semantic Analysis and Intermediate Code Generation

* Allows attaching **semantic actions** to grammar rules.  
* Supports type checking, scope resolution, and generates intermediate code like **Three-Address Code (TAC)**.

**Use in Compiler:** Adds meaning to the parse tree by evaluating expressions and types.

##### **4\. Abstract Syntax Tree (AST) Generators**

* Represent code in a **tree format** where each node is a language construct.  
* ASTs are easier to analyze and optimize than parse trees.

**Use in Compiler:** Basis for semantic analysis and code generation.

##### **5\. Symbol Table Managers**

* A **symbol table** stores information about variables, functions, scopes, and types.  
* Often implemented using **hash tables** or **linked lists**.

**Use in Compiler:** Maintains declarations and identifiers across different scopes.

##### **6\. Intermediate Code Generators**

* Produce machine-independent **Intermediate Representations (IR)** like:  
  * Three-Address Code (TAC)  
  * Quadruples or Triples

**Use in Compiler:** Makes optimization and platform-independent transformations easier.

##### **7\. Code Optimizers**

**Tools:** LLVM Optimizer, Custom Optimizers

* Applies transformations to improve performance or reduce memory.  
* Examples: Dead code elimination, constant folding, loop optimization.

**Use in Compiler:** Produces efficient intermediate or target code.

##### **8\. Code Generators**

**Tools:** LLVM, GCC backend

* Converts optimized IR into **target machine code**.  
* Takes into account register allocation, instruction selection, and target architecture.

**Use in Compiler:** Final output of compilation‚Äîready to run on hardware.

##### **9\. Error Handling and Debugging Tools**

* Built-in within `Lex` and `Yacc`.  
* Tools like **gdb**, **valgrind**, or **custom debugging modules** assist in identifying compiler bugs.

### **1.5 Applications of Compiler Technology:**

1. **Programming Languages**: Every language (C, C++, Java, Python) has a compiler or interpreter.  
2. **Operating Systems**: Kernel and system programs require optimized compiled code.  
3. **Database Management Systems (DBMS)**: SQL query execution uses a compiler to generate execution plans.  
4. **AI & Machine Learning**: Neural networks use compiled models for fast execution.  
5. **Embedded Systems**: Microcontrollers (Arduino, Raspberry Pi) require compilers for code translation.  
6. **Web Browsers:** JavaScript engines like **V8 (Chrome)** and **SpiderMonkey (Firefox)** compile JS code into machine code for fast execution.  
7. **Security & Malware Analysis:** Compilers help in reverse engineering malware.  
8. **Cloud Computing & Virtual Machines:** Cloud platforms (AWS, Google Cloud) use Just-In-Time (JIT) compilers to optimize execution.  
9. **High-Performance Computing (HPC):** Supercomputers and scientific simulations rely on optimized compilers to enhance execution speed and resource efficiency.  
10. **Game Development:** Modern game engines (Unity, Unreal Engine) use compilers to optimize graphics rendering and physics calculations for smooth gameplay.

---

## **Section 2 : Lexical Analysis**

### **2.1 Role of the Lexical Analyzer:**

The **Lexical Analyzer** is the first phase of a compiler. It reads the source code **character by character** and groups them into **tokens**.

#### **Functions of Lexical Analyzer:**

* Converts source code into **tokens** (keywords, identifiers, literals, etc.).  
* Removes **whitespaces, comments, and newlines**.  
* Detects **lexical errors** (invalid characters, unclosed strings).  
* Passes tokens to the **syntax analyzer (parser)**.

> Input Source Code ‚Üí Character Stream ‚Üí Lexical Analyzer ‚Üí Token Stream

üîπ **Example**: 

* For this input (C statement): `int sum = a + b;`  
* Lexical Analyzer Output (Tokens generated):  
  * `int` (Keyword)  
  * `sum` (Identifier)  
  * `=` (Assignment Operator)  
  * `a`, `b` (Identifiers)  
  * `+` (Operator)

### **2.2 Input Buffering:**

Reading the source code character by character is **slow**. To improve performance, **buffering techniques** are used.

* **Problem**: Character-by-character reading slows down the compilation process.  
* **Solution**: Use **input buffering** to read blocks of text instead of individual characters.  
* **Double Buffering Technique**: Uses two buffers to speed up processing.

#### **Techniques for Efficient Input Handling**

##### **üîπ 1\. Naive Approach (One Character at a Time)**

* Reads **one character** at a time from the source code.  
* **Slow** because of frequent disk I/O.

Example:  
`i ‚Üí n ‚Üí t ‚Üí   ‚Üí a ‚Üí =`

##### **üîπ 2\. Input Buffering (Two Buffer Scheme)**

* Uses **two buffers** to read data in blocks.  
* When one buffer is **being processed**, the other **loads new data**.  
* **Improves speed by reducing I/O operations**.

üìå **Example of Two-Buffer System:**

`Buffer 1: [ i   n   t       a   =   ]`    
`Buffer 2: [ 1   0   ;  ... (next input) ]`

**Advantages:**  
 ‚úî Faster than character-by-character reading.  
 ‚úî Reduces the number of disk accesses.

### **2.3 Specification and Recognition of Tokens:**

A **token** is the **smallest meaningful unit** of a program. The **Lexical Analyzer** identifies tokens using **Regular Expressions** and **Finite Automata**.

#### **Types of Tokens**

| Token Type | Example |
| :---: | :---: |
| **Keywords** | `if`, `while`, `return`, `int` |
| **Identifiers** | `sum`, `totalValue`, `x` |
| **Operators** | `+`, `-`, `*`, `/`, `=` |
| **Literals** | `10`, `3.14`, `'A'`, `"Hello"` |
| **Punctuation/Symbols** | `;`, `()`, `{}` |

#### **Token Recognition Using Regular Expressions**

Regular expressions **define token patterns** in lexical analysis.

| Token | Regular Expression |
| :---: | :---: |
| **Identifier** | `[a-zA-Z_][a-zA-Z0-9_]*` |
| **Integer** | `[0-9]+` |
| **Floating Point** | `[0-9]+\.[0-9]+` |
| **Operators** | `[+\-*/=]` |

* **Tokens**: Label for a sequence of characters. 
* **Pattern**: Regular expression that defines the structure of a token.
* **Lexeme**: Actual string in source code that matches a token pattern.

üîπ **Common Token Patterns**:

* **Identifiers**: `[a-zA-Z_][a-zA-Z0-9_]*`  
* **Numbers**: `[0-9]+(\.[0-9]+)?`  
* **Operators**: `+`, `-`, `*`, `/`, `=`

### **2.4 Design of Lexical Analyzer**

A **Lexical Analyzer (Lexer)** is the **first phase** of a compiler that **scans** the source code and **converts it into tokens**.

#### **Working of a Lexical Analyzer**

The **Lexical Analyzer** follows these steps:

 üîπ **Step 1: Read Input Code** ‚Üí The Lexer reads **character sequences** from source file.  
 üîπ **Step 2: Match Patterns** ‚Üí Uses **Regular Expressions** to recognize tokens.  
 üîπ **Step 3: Generate Tokens** ‚Üí Converts recognized sequences into **tokens**.  
 üîπ **Step 4: Ignore Whitespace & Comments** ‚Üí They are removed.  
 üîπ **Step 5: Send Tokens to Parser** ‚Üí The tokens are passed to the **Syntax Analyzer**.

#### **Components of a Lexical Analyzer**

1. **Input Buffer** ‚Üí Stores input code for efficient reading.  
2. **Scanner** ‚Üí Reads characters, groups them into **lexemes** (words, numbers, symbols).  
3. **Pattern Matcher** ‚Üí Uses **Regular Expressions** to classify lexemes.  
4. **Symbol Table** ‚Üí Stores identifiers (variable names, function names).  
5. **Error Handler** ‚Üí Detects **invalid characters, unclosed strings, or unknown symbols**.

#### **Example: Design of a Lexical Analyzer**

For the input code:

`int sum = a + b * 10;`

üìå **Lexical Analyzer Processing:**

1. **Reads** the input code.  
2. **Identifies Tokens**:  
   * `int` ‚Üí **Keyword**  
   * `sum` ‚Üí **Identifier**  
   * `=` ‚Üí **Operator**  
   * `a` ‚Üí **Identifier**  
   * `+` ‚Üí **Operator**  
   * `b` ‚Üí **Identifier**  
   * `*` ‚Üí **Operator**  
   * `10` ‚Üí **Number**  
   * `;` ‚Üí **Symbol**

**Outputs Token Stream**:  
`(Keyword, int)`  
`(Identifier, sum)`  
`(Operator, =)`  
`(Identifier, a)`  
`(Operator, +)`  
`(Identifier, b)`  
`(Operator, *)`  
`(Number, 10)`  
`(Symbol, ;)`

#### **Implementation of Lexical Analyzer Using Lex (Lex Tool Example)**

üîπ **Lex is a tool for creating lexical analyzers in C.**  
üîπ **Example Lex Code for Identifiers and Numbers:**

`%{`  
`#include <stdio.h>`  
`%}`  
`%%`  
`[a-zA-Z_][a-zA-Z0-9_]*   { printf("IDENTIFIER\n"); }`  
`[0-9]+                   { printf("NUMBER\n"); }`  
`"+" | "-" | "*" | "/"    { printf("OPERATOR\n"); }`  
`%%`  
`int main() {`  
    `yylex();  // Call Lex function`  
    `return 0;`  
`}`

‚úÖ This program detects **identifiers, numbers, and operators** in an input file.

### **2.5 Regular Expressions:**

A **Regular Expression (RegEx)** is a **sequence of characters** that defines a search pattern. In **lexical analysis**, regular expressions help identify **tokens** like keywords, identifiers, numbers, and operators.

 ‚úî **Used in Lexical Analyzers** to match **patterns** in source code.  
 ‚úî Helps in **token recognition** for keywords, variables, numbers, etc.  
 ‚úî Can be **converted into finite automata** for efficient processing.

#### **Basic Components of Regular Expressions**

Regular expressions use **symbols** to represent different types of character sequences.

| Symbol | Meaning | Example | Matches |
| ----- | ----- | ----- | ----- |
| `a, b, c...` | Single character | `a` | `a` |
| `.` | Any single character | `c.t` | `cat`, `cut` |
| `*` | 0 or more repetitions | `a*` | `""`, `a`, `aa`, `aaa` |
| `+` | 1 or more repetitions | `a+` | `a`, `aa`, `aaa` |
| `?` | 0 or 1 occurrence | `a?b` | `b`, `ab` |
| `[ ]` | Character set | `[abc]` | `a`, `b`, `c` |
| `[^ ]` | Negation of set | `[^abc]` | Any character except `a, b, c` |
| `[a-z]` | Range of characters | `[a-z]` | Any lowercase letter |
| `( )` | Grouping | `(ab)+` | `ab`, `abab`, `ababab` |
| \` | \` | OR operator | \`a |

#### **Regular Expressions for Different Token Types**

##### **1\. Keywords**

* **Definition:** Fixed words in a programming language.  
* **Regular Expression:** if | else | while | for | return | int | float  
* **Example Matches:** `if`, `else`, `while`, `int`

##### **2\. Identifiers (Variable Names & Function Names)**

* **Definition:** Names given to variables, functions, classes, etc.  
* **Rules:**  
  * Can contain **letters (a-z, A-Z), digits (0-9), and underscores (\_)**   
  * Cannot start with a **digit**  
* **Regular Expression:** \[a-zA-Z\_\]\[a-zA-Z0-9\_\]\*  
* **Example Matches:** `sum`, `total_value`, `_main`, `A12`  
* **Does NOT Match:** `123abc`, `9num` (Because they start with a digit)

##### **3\. Constants (Numbers)**

* **Definition:** Numeric values used in calculations.

###### ***a) Integer Numbers***

* **Regular Expression:** \[0-9\]+  
* **Example Matches:** `10`, `500`, `0`, `123456`

###### ***b) Floating Point Numbers***

* **Regular Expression:** \[0-9\]+\\.\[0-9\]+  
* **Example Matches:** `3.14`, `0.5`, `100.00`

##### **4\. Operators**

* **Definition:** Mathematical, logical, and relational operators.  
* **Regular Expression:** \[+\\-\*/=\<\>\!\]  
* **Example Matches:** `+`, `-`, `*`, `/`, `=`, `<`, `>`

##### **5\. Strings (Text in Quotes)**

* **Definition:** Sequence of characters enclosed in quotes.  
* **Regular Expression:** ".\*"  
* **Example Matches:** `"hello"`, `"123"`, `"C programming"`

#### **Implementation of Regular Expressions in a Lexical Analyzer**

‚úî **Lex tool (Lexical Analyzer Generator)** helps convert **regular expressions into token generators.**  
‚úî Example **Lex Program to Identify Identifiers and Numbers**

`%{`  
`#include <stdio.h>`  
`%}`  
`%%`  
`[a-zA-Z_][a-zA-Z0-9_]*    { printf("IDENTIFIER\n"); }`  
`[0-9]+                    { printf("NUMBER\n"); }`  
`"+" | "-" | "*" | "/"      { printf("OPERATOR\n"); }`  
`%%`  
`int main() {`  
    `yylex();  // Call Lex function`  
    `return 0;`  
`}`

‚úÖ **This Lex program automatically recognizes identifiers, numbers, and operators**

### **2.6 Finite Automata:**

A **Finite Automaton (FA)** is a **mathematical model** used to **recognize patterns** in text, such as **tokens** in a programming language. It helps a **Lexical Analyzer** identify **keywords, identifiers, numbers, and operators**.

‚úî **Finite Automata is used in Lexical Analysis** to match input with predefined **patterns**.  
‚úî It can be represented using **state diagrams** and **transition tables**.  
‚úî **Regular Expressions are converted into Finite Automata** for efficient matching.

#### **Types of Finite Automata**

Finite Automata can be of **two types**:

| Type | Definition | Example |
| ----- | ----- | ----- |
| **1\. DFA (Deterministic Finite Automata)** | One transition per input symbol, no Œµ (empty) transitions. | Used for fast lexical analysis in compilers. |
| **2\. NFA (Nondeterministic Finite Automata)** | Multiple transitions per input symbol, allows Œµ (empty) transitions. | Easier to design but requires conversion to DFA. |

##### **1\. Deterministic Finite Automata (DFA)**

 ‚úî **Each state has exactly one transition per input symbol.**  
 ‚úî **No Œµ (empty) transitions are allowed.**  
 ‚úî Used in **lexical analyzers** for **fast pattern matching**.

###### ***DFA Example: Recognizing Binary Strings (0s and 1s)***

üìå **DFA for the Regular Expression `0+1*` (Strings starting with `0`)**

   (q0) \--0--\> (q1) \--0,1--\> (q1) 

| State | Input 0 | Input 1 | Final State? |
| :---: | :---: | :---: | :---: |
| `q0` (Start) | ‚Üí `q1` | ‚ùå (Invalid) | No |
| `q1` | `q1` | `q1` | ‚úÖ Yes |

‚úî Accepts `0`, `01`, `0001`, `01010`, etc.  
üö´ Rejects `1`, `110`, `101`.

##### **2\. Non-deterministic Finite Automata (NFA)**

 ‚úî **Allows multiple transitions for the same input symbol.**  
 ‚úî **Can have Œµ (empty) transitions**, meaning it can move to a new state without consuming input.  
 ‚úî Easier to construct than DFA but needs **conversion to DFA** for practical use.

###### ***NFA Example: Recognizing ab\* (Strings starting with a, followed by zero or more bs)***

   (q0) \--a--\> (q1) \--b--\> (q1) 

| State | Input `a` | Input `b` | Œµ-Transition | Final State? |
| ----- | ----- | ----- | ----- | ----- |
| `q0` (Start) | ‚Üí `q1` | ‚ùå (Invalid) | ‚ùå | No |
| `q1` | ‚ùå | `q1` | ‚ùå | ‚úÖ Yes |

‚úî Accepts `a`, `ab`, `abb`, `abbbb`, etc.  
 üö´ Rejects `b`, `ba`, `bb`.

#### **Conversion: NFA to DFA**

Since **NFA is less efficient**, it is converted into **DFA** using the **Subset Construction Method**.

 ‚úî **DFA ensures one transition per input** ‚Üí makes processing **faster**.  
 ‚úî Example: **Regular Expression `a(b|c)*`**

* **NFA:** Can go to `b` or `c` from `a`.  
* **DFA:** Creates **separate states** for each possibility (`ab*`, `ac*`).

üìå **Lexical Analyzers use DFA because it's faster and easier to implement.**

#### **Finite Automata in Lexical Analysis**

 ‚úî **Regular Expressions ‚Üí NFA ‚Üí DFA** ‚Üí Used for token recognition.  
 ‚úî Every **token type** (keywords, identifiers, numbers) has a **separate DFA**.  
 ‚úî DFA **eliminates backtracking**, making **compilation efficient**.

##### **Example: DFA for Identifiers in C (\[a-zA-Z\_\]\[a-zA-Z0-9\_\]\*)**

üìå **Valid Identifiers:** `sum`, `total_1`, `_count`  
üìå **Invalid Identifiers:** `1variable`, `@test`, `#num`

###### ***DFA Diagram for Identifiers***

   (q0) \--\[a-zA-Z\_\]--\> (q1) \--\[a-zA-Z0-9\_\]\*--\> (q1)

| State | Input: Letter (`a-z, A-Z, _`) | Input: Digit (`0-9`) | Other Characters (`@, #, etc.`) | Final State? |
| ----- | ----- | ----- | ----- | ----- |
| `q0` (Start) | ‚Üí `q1` | ‚ùå (Invalid) | ‚ùå (Invalid) | No |
| `q1` | `q1` | `q1` | ‚ùå (Invalid) | ‚úÖ Yes |

‚úî Accepts: `sum`, `total_1`, `_count`  
üö´ Rejects: `1variable`, `@test`, `#num`

### **2.7 Conversion Between Regular Expressions and Finite Automata:**

* Regular expressions can be converted to NFA using **Thompson‚Äôs construction**  
* NFA can then be converted to DFA for easier implementation using **subset construction method**.

#### **Why Convert Regular Expressions to Finite Automata?**

‚úî **Lexical Analyzers use Finite Automata** to recognize tokens efficiently.  
 ‚úî **Regular Expressions define token patterns**, but **Finite Automata implement them** for fast matching.  
 ‚úî The **conversion process** allows compilers to **automatically generate lexers** from regular expressions.

#### **Steps to Convert Regular Expressions to Finite Automata**

1. **Convert Regular Expression to an NFA (Nondeterministic Finite Automaton)**  
2. **Convert NFA to DFA (Deterministic Finite Automaton)**  
3. **Minimize DFA** for efficiency

##### **Step 1: Convert Regular Expression to NFA (Thompson's Construction)**

üìå **Thompson's Construction** is used to build an **NFA** from a **regular expression**.

‚úî **NFA allows multiple transitions for the same input** (non-deterministic).  
‚úî Uses **Œµ-transitions** (empty transitions) for flexibility.

###### ***Basic Rules for Constructing NFA***

| Regular Expression | NFA Representation |
| :---- | :---- |
| a | A single state transition for a. |
| \`a | b\` (Union) |
| ab (Concatenation) | Connect a state to b state. |
| a\* (Kleene Star) | Adds loops for zero or more repetitions of a. |

###### ***Example 1: NFA for ab (Concatenation)***

(q0) \--a--\> (q1) \--b--\> (q2)

‚úî Accepts `ab`, but not `a` or `b`.

##### **Step 2: Convert NFA to DFA (Subset Construction Method)**

üìå **NFA is converted to DFA because DFA is faster and easier to implement.**

‚úî **NFA has multiple transitions for the same input** ‚Üí DFA has **only one transition per input**.  
 ‚úî **Œµ-transitions are removed**.  
 ‚úî The **Subset Construction Method** is used to create **deterministic states**.

###### ***Example 2: NFA for (a|b)c***

(q0) \--a--\> (q1) \--c--\> (q3)

(q0) \--b--\> (q2) \--c--\> (q3)

üöÄ **Converting to DFA**:

* `q0` has two possible transitions (`q1` or `q2`).  
* Create a **new DFA state (`q1q2`)** that combines them.  
* Resulting DFA ensures **only one transition per input**.

##### **Step 3: Minimize DFA**

üìå **Minimization reduces the number of states, making lexical analysis faster.**

‚úî **Equivalent states are merged**.  
 ‚úî **Unreachable states are removed**.  
 ‚úî **Final DFA is optimized for performance**.

###### ***Example 3: Minimizing DFA for Identifiers (\[a-zA-Z\_\]\[a-zA-Z0-9\_\]\*)***

1. Initial DFA contains **extra states for each character transition**.  
2. Equivalent states (`q2, q3, q4, ...`) are merged into a **single state**.  
3. The minimized DFA recognizes **identifiers efficiently** with fewer transitions.

#### **Practical Example in a Lexical Analyzer**

‚úî **Lex (Lexical Analyzer Generator)** uses **regular expressions** to create **Finite Automata** for token recognition.

##### **Lex Program Example for Identifiers and Numbers**

`%{`  
`#include <stdio.h>`  
`%}`  
`%%`  
`[a-zA-Z_][a-zA-Z0-9_]*    { printf("IDENTIFIER\n"); }`  
`[0-9]+                    { printf("NUMBER\n"); }`  
`"+" | "-" | "*" | "/"      { printf("OPERATOR\n"); }`  
`%%`  
`int main() {`  
    `yylex();  // Call Lex function`  
    `return 0;`  
`}`

‚úî Lex **automatically converts regular expressions into an optimized DFA** for fast lexical analysis.

### **2.8 Minimizing DFA States:**

DFA can be optimized by reducing the number of states without changing functionality using **state minimization algorithms**.

#### **Why Minimize DFA?**

‚úî **DFA Minimization** reduces the number of states, making lexical analysis **faster and more efficient**.  
‚úî A **smaller DFA** consumes **less memory** and executes **faster**.  
‚úî **Unnecessary states are removed**, but the DFA **still recognizes the same language**.

#### **Steps to Minimize DFA**

1Ô∏è‚É£ **Remove Unreachable States** (States that are never reached from the start state).  
2Ô∏è‚É£ **Merge Equivalent States** (States that behave identically for all inputs).  
3Ô∏è‚É£ **Construct the Minimized DFA** using the **Partition Method** or **Table-Filling Method**.

##### **Step 1: Removing Unreachable States**

‚úî If a state **has no incoming transitions**, or if no path leads to it from the start state, it can be **removed**.  
 ‚úî Example:

(q0) \--a--\> (q1) \--b--\> (q2)  ‚úÖ (reachable)

(q3) \--b--\> (q4)              ‚ùå (unreachable)

üìå **Remove `q3` and `q4` since they are never reached.**

##### **Step 2: Merging Equivalent States**

‚úî **Two states are equivalent if** they always lead to the same output for any input.  
 ‚úî These states can be **merged into one** to simplify the DFA.

###### ***Example DFA Before Minimization***

(q0) \--a--\> (q1) \--b--\> (q2)

(q0) \--b--\> (q3) \--b--\> (q2)

(q1) \--b--\> (q2)

(q3) \--b--\> (q2)

üìå **Observation:**

* `q1` and `q3` behave the same way ‚Üí They can be **merged**.  
* The minimized DFA now has **fewer states**.

##### **Step 3: Constructing the Minimized DFA (Partition Method)**

The **Partition Method** is a systematic way to group equivalent states and merge them.

###### ***Partition Method Steps***

1. **Group states into two sets**:  
   * **Final states** (accepting states).  
   * **Non-final states** (non-accepting states).  
2. **Check each state's behavior**: If two states transition to different groups, they are not equivalent.  
3. **Keep splitting states into smaller groups until no further split is possible.**  
4. **Each final group represents a state in the minimized DFA.**

##### **Alternative: Table-Filling Method**

‚úî Uses a **state table** to check **which states are distinguishable**.  
 ‚úî The table helps eliminate **redundant states** step by step.

#### **Example: DFA Minimization**

**Given DFA:**

States: {q0, q1, q2, q3}  

Final States: {q2}  

Transitions:  

(q0) \--a--\> (q1)    
(q0) \--b--\> (q3)    
(q1) \--b--\> (q2)    
(q3) \--b--\> (q2)  

üìå **Minimized DFA:**

* `q1` and `q3` behave identically ‚Üí Merge them.  
* Final minimized DFA: `{q0, q1q3, q2}`.

### **2.9 Implementation of Lexical Analyzer:**

#### **Steps to Implement a Lexical Analyzer**

 1Ô∏è‚É£ **Read the source code** character by character.  
 2Ô∏è‚É£ **Match character sequences** to token patterns using **Regular Expressions**.  
 3Ô∏è‚É£ **Generate tokens** (keywords, identifiers, numbers, etc.).  
 4Ô∏è‚É£ **Ignore whitespace and comments**.  
 5Ô∏è‚É£ **Handle lexical errors** (invalid characters, unclosed strings).  
 6Ô∏è‚É£ **Pass tokens to the syntax analyzer (parser)**.

#### **Lexical Analyzer Implementation Methods**

**1\. Manual Implementation (Hardcoded in C, Java, Python, etc.)**  
**2\. Using Lexical Analyzer Generators (Lex, Flex, ANTLR, etc.)**

##### **1\. Manual Implementation (Using C)**

###### ***Example: Simple Lexer in C***

`#include <stdio.h>`  
`#include <ctype.h>`  
`#include <string.h>`

`void identify_token(char *word) {`  
    `// Keywords list`  
    `char *keywords[] = {"int", "float", "if", "else", "return"};`  
    `int num_keywords = 5;`  
      
    `// Check if the word is a keyword`  
    `for (int i = 0; i < num_keywords; i++) {`  
        `if (strcmp(word, keywords[i]) == 0) {`  
            `printf("Keyword: %s\n", word);`  
            `return;`  
        `}`  
    `}`

    `// If not a keyword, it's an identifier`  
    `printf("Identifier: %s\n", word);`  
`}`

`void lexical_analyzer(char *code) {`  
    `char buffer[50];`  
    `int i = 0, j = 0;`

    `while (code[i] != '\0') {`  
        `if (isalnum(code[i])) {`  
            `buffer[j++] = code[i];`  
        `} else {`  
            `if (j > 0) {`  
                `buffer[j] = '\0';`  
                `identify_token(buffer);`  
                `j = 0;`  
            `}`  
            `if (ispunct(code[i])) {`  
                `printf("Operator/Symbol: %c\n", code[i]);`  
            `}`  
        `}`  
        `i++;`  
    `}`  
`}`

`int main() {`  
    `char code[] = "int sum = 10 + 20;";`  
    `printf("Lexical Analysis Output:\n");`  
    `lexical_analyzer(code);`  
    `return 0;`  
`}`

###### ***Output:***

`Lexical Analysis Output:`  
`Keyword: int`  
`Identifier: sum`  
`Operator/Symbol: =`  
`Identifier: 10`  
`Operator/Symbol: +`  
`Identifier: 20`  
`Operator/Symbol: ;`

##### **2\. Using Lex Tool (Lex Program for Lexical Analysis)**

‚úî **Lex is a lexical analyzer generator** that automatically **creates lexers** from **regular expressions**.  
 ‚úî It generates **C code** that scans and **tokenizes input**.

###### ***Lex Program Example***

`%{`  
`#include <stdio.h>`  
`%}`  
`%%`  
`"int"|"float"|"if"|"else"|"return"   { printf("Keyword: %s\n", yytext); }`  
`[a-zA-Z_][a-zA-Z0-9_]*               { printf("Identifier: %s\n", yytext); }`  
`[0-9]+                               { printf("Number: %s\n", yytext); }`  
`[+\-*/=;]                            { printf("Operator/Symbol: %s\n", yytext); }`  
`%%`  
`int main() {`  
    `yylex(); // Calls the lexer`  
    `return 0;`  
`}`

###### ***Steps to Run Lex Code:***

1. Save the code as `lexer.l`.  
2. Compile using:  
   `lex lexer.l`  
   `gcc lex.yy.c -o lexer -ll`  
3. Run the lexer:  
    `./lexer < input.txt`  
4. Lex automatically converts regular expressions into a DFA-based scanner.